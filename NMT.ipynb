{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "PceQWlkgcQXW",
        "X1OGqO29cJPK"
      ],
      "mount_file_id": "1UW7O9jP9Kxe-V2RjAbRkr-iOA8GpymG4",
      "authorship_tag": "ABX9TyMEejCNoQoaDU7H6//l6ja7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anup8777/TensorFlow_DeepLearning/blob/master/NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oifNzUVjdzjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wugtLMGVf9tG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flag = \"kan-eng\" # deu -eng\n",
        "flag2 = \"deu-eng\"\n",
        "eng_alphabet = False\n",
        "path_to_file = \"/content/drive/My Drive/IDL Project/\"+flag"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SWbc-aKzPaz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/IDL Project/'+flag)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLTcL2jNg2K_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VE4JDUkjNFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  \n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  if eng_alphabet:\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w) ##########\n",
        "\n",
        "  # the language does not contain english alphabets\n",
        "  \n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar2UxEaikNeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  \n",
        "  \n",
        "  return zip(*word_pairs)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3HkPsu3mT3f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOJfZe062GVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "  # creating cleaned input, output pairs\n",
        "  targ_lang, inp_lang,_ = create_dataset(path, num_examples)\n",
        "\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYozJQWG23De",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh1IFHp73KbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBOXMwqX3XWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qc-WHB1t3hPM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PceQWlkgcQXW",
        "colab_type": "text"
      },
      "source": [
        "## Kannada - English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Epw6PG0jY08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9b367687-2159-42f6-bbec-257e70683104"
      },
      "source": [
        "kan_sentence = u\"ಟಾಮ್ ಏಳಿದನು.\"\n",
        "eng_sentence = u\"Tom woke up.\"\n",
        "\n",
        "print(preprocess_sentence(kan_sentence))\n",
        "print(preprocess_sentence(eng_sentence))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> ಟಾಮ ಏಳದನು . <end>\n",
            "<start> tom woke up . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvQHxntTkUq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81fee526-c9c7-4fec-c478-f7308a9d988b"
      },
      "source": [
        "eng, kan, _ = create_dataset(path_to_file+'/kan.txt', None)\n",
        "print(en[-1])\n",
        "print(kan[-1])"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> i forgot tom was the one who taught you how to play the guitar . <end>\n",
            "<start> ಟಾಮ ನಮಗ ಗಟಾರ ಭಾರಸುವುದಕಕ ಹೕಳಕೂಟಟನಂದು ನನಗ ಮರತುಹೂೕಯತು . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOMq7udNm98w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = len(en)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnlhBO0izxXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file+'/kan.txt', num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsBqh5Y32B3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6efe3c4d-375b-4717-8a57-8c46ccbe272b"
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "119 119 30 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLP2xysK29IE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "63b1713b-68a2-4dfe-951a-73306d5fdefc"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "6 ----> ಟಾಮ\n",
            "159 ----> ಒಬಬ\n",
            "66 ----> ಒಳಳಯ\n",
            "160 ----> ಪರತಸಪರಧ\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "8 ----> tom\n",
            "12 ----> is\n",
            "10 ----> a\n",
            "182 ----> real\n",
            "183 ----> competitor\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0TkhSBH2-bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 16\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 64\n",
        "units = 512\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9l0dcjl3H9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c20d6b2b-01c9-4fde-de79-4a1af525240c"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([16, 12]), TensorShape([16, 19]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MeYJh6C3Vis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "70278bfc-dd15-41e2-e677-2295289b83fb"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (16, 12, 512)\n",
            "Encoder Hidden state shape: (batch size, units) (16, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_4vPKwS3fdz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "56bc7408-9891-405a-a7ad-15f319f7e8ad"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (16, 512)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (16, 12, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io8HtKvQ3lq7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bd1d9817-3eba-475a-f036-0fff9a945eef"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (16, 420)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyW-SeGY3nWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGzCrX0K3qT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZBW5SA83sc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_0vVArP3vnb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "65181828-b072-484e-951d-2305f7618284"
      },
      "source": [
        "EPOCHS = 100\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.7620\n",
            "Epoch 1 Loss 2.6798\n",
            "Time taken for 1 epoch 27.209603309631348 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.3043\n",
            "Epoch 2 Loss 2.3004\n",
            "Time taken for 1 epoch 1.0900146961212158 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.7913\n",
            "Epoch 3 Loss 2.1889\n",
            "Time taken for 1 epoch 0.9031939506530762 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.2619\n",
            "Epoch 4 Loss 2.1670\n",
            "Time taken for 1 epoch 1.1028189659118652 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.2556\n",
            "Epoch 5 Loss 2.1314\n",
            "Time taken for 1 epoch 0.9004545211791992 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.9948\n",
            "Epoch 6 Loss 2.1160\n",
            "Time taken for 1 epoch 1.107140302658081 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.9865\n",
            "Epoch 7 Loss 2.0769\n",
            "Time taken for 1 epoch 0.8937926292419434 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.1183\n",
            "Epoch 8 Loss 2.0384\n",
            "Time taken for 1 epoch 1.1050126552581787 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.9196\n",
            "Epoch 9 Loss 1.9988\n",
            "Time taken for 1 epoch 0.9042630195617676 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.0915\n",
            "Epoch 10 Loss 1.9853\n",
            "Time taken for 1 epoch 1.1213581562042236 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.0385\n",
            "Epoch 11 Loss 1.9469\n",
            "Time taken for 1 epoch 0.8946478366851807 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.1830\n",
            "Epoch 12 Loss 1.9159\n",
            "Time taken for 1 epoch 1.1129653453826904 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.0626\n",
            "Epoch 13 Loss 1.8653\n",
            "Time taken for 1 epoch 0.900179386138916 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.9948\n",
            "Epoch 14 Loss 1.8481\n",
            "Time taken for 1 epoch 1.0967698097229004 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.8275\n",
            "Epoch 15 Loss 1.7828\n",
            "Time taken for 1 epoch 0.9168529510498047 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.7502\n",
            "Epoch 16 Loss 1.7423\n",
            "Time taken for 1 epoch 1.1844279766082764 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.7182\n",
            "Epoch 17 Loss 1.7119\n",
            "Time taken for 1 epoch 0.886974573135376 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.6346\n",
            "Epoch 18 Loss 1.6701\n",
            "Time taken for 1 epoch 1.099494218826294 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.7241\n",
            "Epoch 19 Loss 1.6001\n",
            "Time taken for 1 epoch 0.9070277214050293 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.4022\n",
            "Epoch 20 Loss 1.5515\n",
            "Time taken for 1 epoch 1.0956909656524658 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.7644\n",
            "Epoch 21 Loss 1.5249\n",
            "Time taken for 1 epoch 0.897791862487793 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.5018\n",
            "Epoch 22 Loss 1.4819\n",
            "Time taken for 1 epoch 1.1269724369049072 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.5324\n",
            "Epoch 23 Loss 1.4176\n",
            "Time taken for 1 epoch 0.9028670787811279 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.3434\n",
            "Epoch 24 Loss 1.3870\n",
            "Time taken for 1 epoch 1.078537940979004 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.4397\n",
            "Epoch 25 Loss 1.3455\n",
            "Time taken for 1 epoch 0.8927469253540039 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.3226\n",
            "Epoch 26 Loss 1.3011\n",
            "Time taken for 1 epoch 1.2009551525115967 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.0565\n",
            "Epoch 27 Loss 1.2627\n",
            "Time taken for 1 epoch 0.9167623519897461 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.0861\n",
            "Epoch 28 Loss 1.1920\n",
            "Time taken for 1 epoch 1.1343810558319092 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.0963\n",
            "Epoch 29 Loss 1.1774\n",
            "Time taken for 1 epoch 0.8926839828491211 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.1804\n",
            "Epoch 30 Loss 1.1162\n",
            "Time taken for 1 epoch 1.1135854721069336 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.2439\n",
            "Epoch 31 Loss 1.0870\n",
            "Time taken for 1 epoch 0.8926167488098145 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.9999\n",
            "Epoch 32 Loss 1.0562\n",
            "Time taken for 1 epoch 1.1092405319213867 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 1.0472\n",
            "Epoch 33 Loss 1.0159\n",
            "Time taken for 1 epoch 0.8977656364440918 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 1.0280\n",
            "Epoch 34 Loss 0.9941\n",
            "Time taken for 1 epoch 1.0963139533996582 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.8497\n",
            "Epoch 35 Loss 0.9507\n",
            "Time taken for 1 epoch 0.9111418724060059 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 1.0517\n",
            "Epoch 36 Loss 0.9118\n",
            "Time taken for 1 epoch 1.1236588954925537 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.8358\n",
            "Epoch 37 Loss 0.8560\n",
            "Time taken for 1 epoch 0.8985855579376221 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.6780\n",
            "Epoch 38 Loss 0.8118\n",
            "Time taken for 1 epoch 1.1593670845031738 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.7392\n",
            "Epoch 39 Loss 0.7851\n",
            "Time taken for 1 epoch 0.8911490440368652 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.7301\n",
            "Epoch 40 Loss 0.7484\n",
            "Time taken for 1 epoch 1.1593246459960938 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.6956\n",
            "Epoch 41 Loss 0.7002\n",
            "Time taken for 1 epoch 0.8931307792663574 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.7416\n",
            "Epoch 42 Loss 0.6952\n",
            "Time taken for 1 epoch 1.1189212799072266 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.5657\n",
            "Epoch 43 Loss 0.6761\n",
            "Time taken for 1 epoch 0.9086740016937256 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.6042\n",
            "Epoch 44 Loss 0.6211\n",
            "Time taken for 1 epoch 1.101090431213379 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.5924\n",
            "Epoch 45 Loss 0.6097\n",
            "Time taken for 1 epoch 0.9039909839630127 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.5729\n",
            "Epoch 46 Loss 0.5738\n",
            "Time taken for 1 epoch 1.1534326076507568 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.6233\n",
            "Epoch 47 Loss 0.5607\n",
            "Time taken for 1 epoch 0.901674747467041 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.4309\n",
            "Epoch 48 Loss 0.5366\n",
            "Time taken for 1 epoch 1.1000092029571533 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.4913\n",
            "Epoch 49 Loss 0.4979\n",
            "Time taken for 1 epoch 0.8891787528991699 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.3879\n",
            "Epoch 50 Loss 0.4645\n",
            "Time taken for 1 epoch 1.1845860481262207 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.4371\n",
            "Epoch 51 Loss 0.4594\n",
            "Time taken for 1 epoch 0.8966724872589111 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.5078\n",
            "Epoch 52 Loss 0.4469\n",
            "Time taken for 1 epoch 1.1021523475646973 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.3758\n",
            "Epoch 53 Loss 0.4172\n",
            "Time taken for 1 epoch 0.9122815132141113 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.3933\n",
            "Epoch 54 Loss 0.3821\n",
            "Time taken for 1 epoch 1.0898702144622803 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.3213\n",
            "Epoch 55 Loss 0.3761\n",
            "Time taken for 1 epoch 0.9067306518554688 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.3308\n",
            "Epoch 56 Loss 0.3785\n",
            "Time taken for 1 epoch 1.1304817199707031 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.3542\n",
            "Epoch 57 Loss 0.3460\n",
            "Time taken for 1 epoch 0.9015450477600098 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.3394\n",
            "Epoch 58 Loss 0.3448\n",
            "Time taken for 1 epoch 1.104010820388794 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.3030\n",
            "Epoch 59 Loss 0.3308\n",
            "Time taken for 1 epoch 0.9073104858398438 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.3098\n",
            "Epoch 60 Loss 0.3190\n",
            "Time taken for 1 epoch 1.2075769901275635 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.3482\n",
            "Epoch 61 Loss 0.2996\n",
            "Time taken for 1 epoch 0.8914551734924316 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.3170\n",
            "Epoch 62 Loss 0.2945\n",
            "Time taken for 1 epoch 1.1000866889953613 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.2291\n",
            "Epoch 63 Loss 0.2658\n",
            "Time taken for 1 epoch 0.8859682083129883 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.2094\n",
            "Epoch 64 Loss 0.2518\n",
            "Time taken for 1 epoch 1.1175267696380615 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.1931\n",
            "Epoch 65 Loss 0.2286\n",
            "Time taken for 1 epoch 0.9133491516113281 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.1870\n",
            "Epoch 66 Loss 0.2113\n",
            "Time taken for 1 epoch 1.1309058666229248 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.1669\n",
            "Epoch 67 Loss 0.1976\n",
            "Time taken for 1 epoch 0.9014666080474854 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.1798\n",
            "Epoch 68 Loss 0.1901\n",
            "Time taken for 1 epoch 1.087981939315796 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.1770\n",
            "Epoch 69 Loss 0.1776\n",
            "Time taken for 1 epoch 0.9163398742675781 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.1546\n",
            "Epoch 70 Loss 0.1660\n",
            "Time taken for 1 epoch 1.103461742401123 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.1267\n",
            "Epoch 71 Loss 0.1539\n",
            "Time taken for 1 epoch 0.9216125011444092 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.1427\n",
            "Epoch 72 Loss 0.1449\n",
            "Time taken for 1 epoch 1.0835182666778564 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.1158\n",
            "Epoch 73 Loss 0.1362\n",
            "Time taken for 1 epoch 0.9108872413635254 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.1417\n",
            "Epoch 74 Loss 0.1290\n",
            "Time taken for 1 epoch 1.1819384098052979 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.1586\n",
            "Epoch 75 Loss 0.1254\n",
            "Time taken for 1 epoch 0.8986473083496094 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.0960\n",
            "Epoch 76 Loss 0.1219\n",
            "Time taken for 1 epoch 1.1166656017303467 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.1105\n",
            "Epoch 77 Loss 0.1163\n",
            "Time taken for 1 epoch 0.9179177284240723 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.1241\n",
            "Epoch 78 Loss 0.1116\n",
            "Time taken for 1 epoch 1.0951337814331055 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.1836\n",
            "Epoch 79 Loss 0.1201\n",
            "Time taken for 1 epoch 0.914067268371582 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.1189\n",
            "Epoch 80 Loss 0.1216\n",
            "Time taken for 1 epoch 1.1446144580841064 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.1148\n",
            "Epoch 81 Loss 0.1139\n",
            "Time taken for 1 epoch 0.8997597694396973 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.1464\n",
            "Epoch 82 Loss 0.1324\n",
            "Time taken for 1 epoch 1.1293611526489258 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.1610\n",
            "Epoch 83 Loss 0.1273\n",
            "Time taken for 1 epoch 0.9025821685791016 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.1219\n",
            "Epoch 84 Loss 0.1161\n",
            "Time taken for 1 epoch 1.0855350494384766 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.1021\n",
            "Epoch 85 Loss 0.1142\n",
            "Time taken for 1 epoch 0.8937456607818604 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.0664\n",
            "Epoch 86 Loss 0.0977\n",
            "Time taken for 1 epoch 1.1418452262878418 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0760\n",
            "Epoch 87 Loss 0.0818\n",
            "Time taken for 1 epoch 0.8894696235656738 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0545\n",
            "Epoch 88 Loss 0.0750\n",
            "Time taken for 1 epoch 1.1376025676727295 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0640\n",
            "Epoch 89 Loss 0.0666\n",
            "Time taken for 1 epoch 0.8987901210784912 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.0568\n",
            "Epoch 90 Loss 0.0610\n",
            "Time taken for 1 epoch 1.0989556312561035 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0506\n",
            "Epoch 91 Loss 0.0538\n",
            "Time taken for 1 epoch 0.8826367855072021 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.0594\n",
            "Epoch 92 Loss 0.0514\n",
            "Time taken for 1 epoch 1.1379592418670654 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0364\n",
            "Epoch 93 Loss 0.0467\n",
            "Time taken for 1 epoch 0.8856780529022217 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.0433\n",
            "Epoch 94 Loss 0.0426\n",
            "Time taken for 1 epoch 1.1927342414855957 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0325\n",
            "Epoch 95 Loss 0.0389\n",
            "Time taken for 1 epoch 0.902705192565918 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.0477\n",
            "Epoch 96 Loss 0.0382\n",
            "Time taken for 1 epoch 1.1221644878387451 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0407\n",
            "Epoch 97 Loss 0.0345\n",
            "Time taken for 1 epoch 0.8916022777557373 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.0281\n",
            "Epoch 98 Loss 0.0321\n",
            "Time taken for 1 epoch 1.101775884628296 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0245\n",
            "Epoch 99 Loss 0.0301\n",
            "Time taken for 1 epoch 0.8917789459228516 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0240\n",
            "Epoch 100 Loss 0.0285\n",
            "Time taken for 1 epoch 1.1207053661346436 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02L1q-q73xJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fO5fV6x30pE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZoNpkzL34dM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "  print('res',result)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXMDoyYK36vs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c488704a-997d-4823-d402-6582f966cc8b"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f49ffa10ef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IEs5yTz39rE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "828e1613-a53b-4001-aee6-323a1841e5bc"
      },
      "source": [
        "translate(u'ಅರ್ಧ ನನಗೆ ಕೊಡು.')"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "res give me half . <end> \n",
            "Input: <start> ಅರಧ ನನಗ ಕೂಡು . <end>\n",
            "Predicted translation: give me half . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3205 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3248 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3239 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3240 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3223 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3221 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3266 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3233 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3265 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3205 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3248 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3239 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3240 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3223 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3221 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3266 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3233 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3265 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rSYkCZUIKpb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4bbf98e7-2cfe-4794-9708-a8018a834ea1"
      },
      "source": [
        "translate(u'ನಿಮಗೆ ನೆನಪು ಇದ್ಯಾ?')"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "res do you remember ? <end> \n",
            "Input: <start> ನಮಗ ನನಪು ಇದಯಾ ? <end>\n",
            "Predicted translation: do you remember ? <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3240 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3246 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3223 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3242 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3265 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3207 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3238 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3247 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 3262 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3240 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3246 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3223 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3242 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3265 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3207 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3238 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3247 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 3262 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1OGqO29cJPK",
        "colab_type": "text"
      },
      "source": [
        "## Deutsche - English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aae_XpzPcM3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2d5bcc6c-ccc6-4b6b-87e0-8e3f30aeec8e"
      },
      "source": [
        "deu_sentence = u\"Warte.\"\n",
        "en_sentence = u\"Wait.\"\n",
        "\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(deu_sentence).encode('utf-8'))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> wait . <end>\n",
            "b'<start> warte . <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdehMHmPpJoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en, deu, _ = create_dataset(path_to_file+'/deu.txt', None)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnvUgfyupozt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c41adde9-2cdf-48e7-b5bc-42ff75c8094f"
      },
      "source": [
        "print(en[-1])\n",
        "print(deu[-1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman . <end>\n",
            "<start> ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht , dass ein mensch nur gelegenheit hat , mit ein paar hundert anderen bekannt zu sein , von denen ihm nur ein dutzend oder weniger nahesteht , darunter hochstens ein oder zwei freunde , dann erahnt man eingedenk der millionen einwohner dieser welt leicht , dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF9RMUegp3Vx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_examples = int(0.5*len(en))\n",
        "\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file+'/deu.txt', num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GEOiKVEqSKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e607429a-ec52-42a0-b517-6be183633f54"
      },
      "source": [
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "88612 88612 22154 22154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ImEqPi0qt48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "1ac5281b-e24d-4d0e-e434-b57d67c22428"
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "53 ----> warum\n",
            "2254 ----> farben\n",
            "34 ----> sich\n",
            "20 ----> die\n",
            "327 ----> leute\n",
            "20 ----> die\n",
            "441 ----> haare\n",
            "6 ----> ?\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "53 ----> why\n",
            "16 ----> do\n",
            "232 ----> people\n",
            "2306 ----> dye\n",
            "480 ----> their\n",
            "256 ----> hair\n",
            "7 ----> ?\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNUWx94TqxUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZjBYFNvrOIU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "51076e5c-4b57-48f0-da0e-18d4e7313c00"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 24]), TensorShape([64, 13]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKz6XV8OrTJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "bf48623d-99ca-412e-b99e-b1bb53f429fb"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 24, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN-hhPFdu3fr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5acc3376-e4a0-4ae6-b75d-df502f409e7b"
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 24, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aKJLJ4Zu7b5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58d5003f-092f-4a99-9dc2-b9788601bc88"
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 9405)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wtyNP3cu_N6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6NZtpWDvCaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2GvBKuPvESa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnf1DzaivG5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5f5c3e3-f10b-4d4d-fbd4-af9d3bfd8f3c"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 5.0032\n",
            "Epoch 1 Batch 100 Loss 2.4044\n",
            "Epoch 1 Batch 200 Loss 2.0167\n",
            "Epoch 1 Batch 300 Loss 1.8809\n",
            "Epoch 1 Batch 400 Loss 1.6429\n",
            "Epoch 1 Batch 500 Loss 1.7022\n",
            "Epoch 1 Batch 600 Loss 1.5370\n",
            "Epoch 1 Batch 700 Loss 1.5513\n",
            "Epoch 1 Batch 800 Loss 1.3560\n",
            "Epoch 1 Batch 900 Loss 1.3121\n",
            "Epoch 1 Batch 1000 Loss 1.1291\n",
            "Epoch 1 Batch 1100 Loss 1.0845\n",
            "Epoch 1 Batch 1200 Loss 1.2986\n",
            "Epoch 1 Batch 1300 Loss 0.9392\n",
            "Epoch 1 Loss 1.5687\n",
            "Time taken for 1 epoch 419.95623540878296 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.0138\n",
            "Epoch 2 Batch 100 Loss 0.9187\n",
            "Epoch 2 Batch 200 Loss 0.8871\n",
            "Epoch 2 Batch 300 Loss 0.7681\n",
            "Epoch 2 Batch 400 Loss 0.7979\n",
            "Epoch 2 Batch 500 Loss 0.8254\n",
            "Epoch 2 Batch 600 Loss 0.6794\n",
            "Epoch 2 Batch 700 Loss 0.7674\n",
            "Epoch 2 Batch 800 Loss 0.7599\n",
            "Epoch 2 Batch 900 Loss 0.7065\n",
            "Epoch 2 Batch 1000 Loss 0.6547\n",
            "Epoch 2 Batch 1100 Loss 0.6247\n",
            "Epoch 2 Batch 1200 Loss 0.5884\n",
            "Epoch 2 Batch 1300 Loss 0.6024\n",
            "Epoch 2 Loss 0.7468\n",
            "Time taken for 1 epoch 406.8803548812866 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4835\n",
            "Epoch 3 Batch 100 Loss 0.4937\n",
            "Epoch 3 Batch 200 Loss 0.5729\n",
            "Epoch 3 Batch 300 Loss 0.5128\n",
            "Epoch 3 Batch 400 Loss 0.5187\n",
            "Epoch 3 Batch 500 Loss 0.5256\n",
            "Epoch 3 Batch 600 Loss 0.4246\n",
            "Epoch 3 Batch 700 Loss 0.4582\n",
            "Epoch 3 Batch 800 Loss 0.5053\n",
            "Epoch 3 Batch 900 Loss 0.3871\n",
            "Epoch 3 Batch 1000 Loss 0.5027\n",
            "Epoch 3 Batch 1100 Loss 0.5436\n",
            "Epoch 3 Batch 1200 Loss 0.5089\n",
            "Epoch 3 Batch 1300 Loss 0.4715\n",
            "Epoch 3 Loss 0.4589\n",
            "Time taken for 1 epoch 402.94555497169495 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.3462\n",
            "Epoch 4 Batch 100 Loss 0.2718\n",
            "Epoch 4 Batch 200 Loss 0.3318\n",
            "Epoch 4 Batch 300 Loss 0.3366\n",
            "Epoch 4 Batch 400 Loss 0.2321\n",
            "Epoch 4 Batch 500 Loss 0.2778\n",
            "Epoch 4 Batch 600 Loss 0.3551\n",
            "Epoch 4 Batch 700 Loss 0.2190\n",
            "Epoch 4 Batch 800 Loss 0.3643\n",
            "Epoch 4 Batch 900 Loss 0.3678\n",
            "Epoch 4 Batch 1000 Loss 0.2792\n",
            "Epoch 4 Batch 1100 Loss 0.3025\n",
            "Epoch 4 Batch 1200 Loss 0.3651\n",
            "Epoch 4 Batch 1300 Loss 0.2699\n",
            "Epoch 4 Loss 0.3119\n",
            "Time taken for 1 epoch 401.75312662124634 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1817\n",
            "Epoch 5 Batch 100 Loss 0.1851\n",
            "Epoch 5 Batch 200 Loss 0.2094\n",
            "Epoch 5 Batch 300 Loss 0.2272\n",
            "Epoch 5 Batch 400 Loss 0.2384\n",
            "Epoch 5 Batch 500 Loss 0.2143\n",
            "Epoch 5 Batch 600 Loss 0.1634\n",
            "Epoch 5 Batch 700 Loss 0.1488\n",
            "Epoch 5 Batch 800 Loss 0.2875\n",
            "Epoch 5 Batch 900 Loss 0.2211\n",
            "Epoch 5 Batch 1000 Loss 0.2613\n",
            "Epoch 5 Batch 1100 Loss 0.2577\n",
            "Epoch 5 Batch 1200 Loss 0.2665\n",
            "Epoch 5 Batch 1300 Loss 0.2024\n",
            "Epoch 5 Loss 0.2263\n",
            "Time taken for 1 epoch 403.0334966182709 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1637\n",
            "Epoch 6 Batch 100 Loss 0.1444\n",
            "Epoch 6 Batch 200 Loss 0.1767\n",
            "Epoch 6 Batch 300 Loss 0.1867\n",
            "Epoch 6 Batch 400 Loss 0.1301\n",
            "Epoch 6 Batch 500 Loss 0.2080\n",
            "Epoch 6 Batch 600 Loss 0.1692\n",
            "Epoch 6 Batch 700 Loss 0.2190\n",
            "Epoch 6 Batch 800 Loss 0.1818\n",
            "Epoch 6 Batch 900 Loss 0.1754\n",
            "Epoch 6 Batch 1000 Loss 0.1703\n",
            "Epoch 6 Batch 1100 Loss 0.1897\n",
            "Epoch 6 Batch 1200 Loss 0.2253\n",
            "Epoch 6 Batch 1300 Loss 0.1796\n",
            "Epoch 6 Loss 0.1724\n",
            "Time taken for 1 epoch 402.73066997528076 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.1452\n",
            "Epoch 7 Batch 100 Loss 0.1443\n",
            "Epoch 7 Batch 200 Loss 0.1011\n",
            "Epoch 7 Batch 300 Loss 0.1628\n",
            "Epoch 7 Batch 400 Loss 0.1254\n",
            "Epoch 7 Batch 500 Loss 0.1066\n",
            "Epoch 7 Batch 600 Loss 0.1782\n",
            "Epoch 7 Batch 700 Loss 0.1493\n",
            "Epoch 7 Batch 800 Loss 0.1801\n",
            "Epoch 7 Batch 900 Loss 0.1765\n",
            "Epoch 7 Batch 1000 Loss 0.1451\n",
            "Epoch 7 Batch 1100 Loss 0.1229\n",
            "Epoch 7 Batch 1200 Loss 0.1415\n",
            "Epoch 7 Batch 1300 Loss 0.1591\n",
            "Epoch 7 Loss 0.1386\n",
            "Time taken for 1 epoch 402.5476064682007 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1016\n",
            "Epoch 8 Batch 100 Loss 0.1037\n",
            "Epoch 8 Batch 200 Loss 0.1033\n",
            "Epoch 8 Batch 300 Loss 0.0961\n",
            "Epoch 8 Batch 400 Loss 0.0841\n",
            "Epoch 8 Batch 500 Loss 0.1018\n",
            "Epoch 8 Batch 600 Loss 0.1477\n",
            "Epoch 8 Batch 700 Loss 0.1474\n",
            "Epoch 8 Batch 800 Loss 0.0811\n",
            "Epoch 8 Batch 900 Loss 0.1332\n",
            "Epoch 8 Batch 1000 Loss 0.1516\n",
            "Epoch 8 Batch 1100 Loss 0.1063\n",
            "Epoch 8 Batch 1200 Loss 0.1017\n",
            "Epoch 8 Batch 1300 Loss 0.1136\n",
            "Epoch 8 Loss 0.1170\n",
            "Time taken for 1 epoch 404.7842044830322 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0971\n",
            "Epoch 9 Batch 100 Loss 0.0828\n",
            "Epoch 9 Batch 200 Loss 0.0864\n",
            "Epoch 9 Batch 300 Loss 0.1041\n",
            "Epoch 9 Batch 400 Loss 0.0688\n",
            "Epoch 9 Batch 500 Loss 0.1085\n",
            "Epoch 9 Batch 600 Loss 0.1028\n",
            "Epoch 9 Batch 700 Loss 0.1204\n",
            "Epoch 9 Batch 800 Loss 0.1343\n",
            "Epoch 9 Batch 900 Loss 0.1303\n",
            "Epoch 9 Batch 1000 Loss 0.1184\n",
            "Epoch 9 Batch 1100 Loss 0.1055\n",
            "Epoch 9 Batch 1200 Loss 0.1382\n",
            "Epoch 9 Batch 1300 Loss 0.1208\n",
            "Epoch 9 Loss 0.1026\n",
            "Time taken for 1 epoch 402.13426995277405 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0546\n",
            "Epoch 10 Batch 100 Loss 0.0704\n",
            "Epoch 10 Batch 200 Loss 0.0649\n",
            "Epoch 10 Batch 300 Loss 0.0660\n",
            "Epoch 10 Batch 400 Loss 0.0807\n",
            "Epoch 10 Batch 500 Loss 0.0813\n",
            "Epoch 10 Batch 600 Loss 0.0795\n",
            "Epoch 10 Batch 700 Loss 0.1043\n",
            "Epoch 10 Batch 800 Loss 0.1384\n",
            "Epoch 10 Batch 900 Loss 0.1217\n",
            "Epoch 10 Batch 1000 Loss 0.1062\n",
            "Epoch 10 Batch 1100 Loss 0.1240\n",
            "Epoch 10 Batch 1200 Loss 0.1379\n",
            "Epoch 10 Batch 1300 Loss 0.0753\n",
            "Epoch 10 Loss 0.0915\n",
            "Time taken for 1 epoch 402.6829409599304 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BqWl3RAvMK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<end>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nP9EUGEvPzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3z4LYoQ9vWbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2hRVddtvXxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0938d37a-e141-428c-d919-d8ea877e7b7b"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f4a02e1c550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsrI5J8HBSjn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "outputId": "8e7fdd52-2577-4f38-cb39-9b84f3860add"
      },
      "source": [
        "translate(U\"Irgendwas stimmt nicht mit ihm.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> irgendwas stimmt nicht mit ihm . <end>\n",
            "Predicted translation: something isn t stupid with him . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyYuS7N_BgJ2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        },
        "outputId": "548cf06a-6ae4-4cf5-9714-a96568e13d99"
      },
      "source": [
        "translate(u'Irgendwas stimmt hier nicht!')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> irgendwas stimmt hier nicht ! <end>\n",
            "Predicted translation: something doesn t seem right here . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}